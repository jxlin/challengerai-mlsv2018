![](https://habrastorage.org/webt/d9/au/73/d9au73qbi27ithcxbdzn3_vcsj4.jpeg)
3 сентебря в ods.ai скинули ссылки на соревнования на китайской платформе aichalelnger. Артур Кузин [link] предложил всем желающим поучавствовать в short-video realtime classification. Меня заинтересовало данное соревнование, и так образовалась команда из меня и [Ильи Бойцова](https://www.linkedin.com/in/ilya-boytsov-2496a2128/). 

У нас был Артур, его девбокс, куча идей и огромное желание побеждать. Казалось этого достаточно, что бы пошатнуть любой kaggle, но китайская платформа оказалась крепким орешком.

<cut />

## Описание задачи

Необходимо было классифицировать короткие видео в пределах реал-тайма, где каждое видео могло иметь более одного тега. Задача имела несколько особенностей:

1. Налагались ограничения на скорость и точность предсказания.
2. Данных было много.
3. Большая часть видео имела только один тег.
4. Все было на китайском.

Первый пункт был самый интересный, во-первых, не сразу стало ясно, на каком железе будет все считаться, во-вторых, ограничение по скорости так же включало в себя чтение видео. Таким образом можно было выделить две критичные части будущего решения: оптимизированное декодирование видео и сверточная сеть. Забегая вперед, скажу, что мы не уделили первой части должного внимания, за что и поплатились, но об этом ниже.

Предсказать нужно было следующие теги:

标签ID | 标签名称 | Tag Name | 标签ID | 标签名称 | Tag Name | 标签ID | 标签名称 | Tag Name |
--- | --- | --- | --- |--- | --- | --- | --- | --- |
0 | 狗 | Dog | 21 | 芭蕾舞 | Ballet | 42 | 游戏 | Games |
1 | 猫 | Cat | 22 | 广场舞 | Square Dancing | 43 | 娱乐 | Entertainment |
2 | 鼠 | Mouse | 23 | 民族舞 | Folk Dance | 44 | 动漫 | Animation |
3 | 兔子 | Rabbit | 24 | 绘画 | Drawing | 45 | 文字艺术配音 | Word Art Voicing |
4 | 鸟 | Bird | 25 | 手写文字 | Handwriting | 46 | 瑜伽 | Yoga |
5 | 风景 | Scenery | 26 | 咖啡拉花	 | Latte Art | 47 | 健身 | Fitness |
6 | 风土人情 | Local Customs | 27 | 沙画 | Sand Drawing | 48 | 滑板 | Skateboard |
7 | 穿秀 | Dressing | 28 | 史莱姆 | Slime | 49 | 篮球 | Basketball |
8 | 宝宝 | Baby | 29 | 折纸 | Origami | 50 | 跑酷 | Parkour |
9 | 男生自拍 | Selfie-Male | 30 | 编织 | Knitting | 51 | 潜水 | Diving |
10 | 女生自拍	 | Selfie-Female | 31 | 发饰 | Hair Accessory | 52 | 台球 | Billiards |
11 | 做甜品 | Dessert Making | 32 | 陶艺 | Pottery | 53 | 足球 | Football |
12 | 做海鲜 | Seafood Making | 33 | 手机壳 | Phone Case | 54 | 羽毛球 | Badminton |
13 | 街边小吃	 | Streetside Snacks | 34 | 打鼓 | Drums | 55 | 乒乓球 | Table Tennis |
14 | 饮品 | Drinks | 35 | 弹吉他 | Guitar | 56 | 画眉 | Brow Painting |
15 | 火锅 | Hot Pot | 36 | 弹钢琴 | Piano | 57 | 画眼 | Eyeliner |
16 | 抓娃娃 | Claw Crane | 37 | 弹古筝 | Guzheng | 58 | 护肤 | Skincare |
17 | 手势舞 | Handsign Dance | 38 | 拉小提琴 | Violin | 59 | 唇彩 | Lipgloss |
18 | 街舞 | Street Dance | 39 | 拉大提琴	| Cello | 60 | 卸妆 | Makeup Removal |
19 | 国标舞 | International Dance | 40 | 吹葫芦丝	 | Hulusi | 61 | 美甲 | Nail Cosmetic |
20 | 钢管舞 | Pole Dance | 41 | 唱歌 | Singing | 62 | 美发 | Hair Cosmetic |

В качестве метрики использовался усредненный по всем классам индекс Джаккарда (Average Jaccard Index), который определяется как отношение площади пересечения к площади объединения. 

Организаторы задали следующие граничные условия для попадания в ЛБ:

1. Время инференса не должно быть более 120 ms для одного видео.
2. Точность должна быть не менее 0.8

С самого начала соревнования, ЛБ ранжировался только по точности, а графы "время инференса" там и вовсе не было. За пару недель до конца соревнования, орги внезапно сменили метрику на композитную и выкинули из ЛБ всех тех, у кого точность была ниже 0.8 и время инференса более 120 ms. Композитная метрика рассчитывалась как взвешенное расстояние качества от скорости алгоритма в координатах. 

### Данные
Организаторы предоставили train и validation set, в первом было 125368 файлов, а во втором 30736 соответсвенно. В трейне только 11920 видео имели более 1 тега, а в валидации 3043. Видео были короткие и разной длины. Кажется, еще и со случайным звуковым сопровождением.

![](https://habrastorage.org/webt/__/fx/v8/__fxv88nyoaehuhin9nnlenmnjk.jpeg)

 ### Сабмит решения
 Решение нужно было отправлять в виде докер файла. Пример с описанием и базовым API [предоставили](https://github.com/AIChallenger/AI_Challenger_2018/tree/master/Evaluation/short_video_real_time_classification_eval]) организаторы. В итоге, участникам необходимо было встроить свою модель в  [infer.py](https://github.com/AIChallenger/AI_Challenger_2018/blob/master/Evaluation/short_video_real_time_classification_eval/mxnet/infer/infer.py), следуя API. Давалось 3 попытки сабмита в неделю. ЛБ состоял из теста А и Б, при этом докер тестировался только на тесте А, тест Б открывался за 3 дня до конца соревнования и к нему допустались только те участники, решение которых укладывалось в ограничения по точности и скорости. На тест Б можно было отправить два решения, и результаты раскрывались только по окончанию соревнования. 6 первых команд по тесту Б приглашались в Пекин для очной презентации.

## Baseline
Начать мы попытались с довольно стандартной схемы для multi-label classification. Для чтения видео взяли opencv, выбрали seresnext50_32x4d как базовую сеть для классификации и BCEWithLogitsLoss в качестве функции потерь. И тут же столкнулись с проблемами.
![](https://habrastorage.org/webt/ie/4f/_c/ie4f_cnale4hqugbi5i_9alxq7k.png)

Оказалось, что opencv для чтения видео - это очень медленно, а из-за жуткого дисбаланса классов, BCEWithLogitsLoss практически не дает какого-то либо стабильного и хорошего результата. Так как в данных было много видео с одним тегом и предсказание лишнего тега сильно било по точности, мы решили для начала сконцетрироваться на предсказании только одного тега. opencv была заменена на PyAV и BCEWithLogitsLoss на обычный CrossEntropyLoss. Данное решение уже давало 0.78 точности на валидационном сете. Докера еще не было, мы расслабились и предвскушали легкое выход на ЛБ.

## Предпроцессинг и подготовка данных
Наш baseline был достаточно простым, мы брали первый кадр каждого видео, применяли стандартные аугментации и подавали на вход сети, получив более менее стабильный результат, решили попробовать брать первые N кадров из видео или T случайных кадров. Второй вариант осказался более предпочтительным, т.к. если видео включало в себя несколько тегов, то объекты, соответсвующие этим тегам, могли находиться в разных временных промежутках. Была идея использовать аудио канал, но особенностью собранных данных была наложенная музыка, которая часто не имела ничего общего с происходящим.

## Наше решение

Мы не стали останавливаться на одной сетке, и накинули дополнительных классификаторов. В итоге, для обучения сети и классикаторов был выстроен следующий пайплайн. Из видео извлекалось 3 случайных кадров, применялись стандартные аугментации и кадры подавались на вход сети. Сеть давала усредненное предсказание. Затем мы использовали обученную сеть для извлечения признаков из видео, которые брались после global pooling, и последующего обучения классикаторов. В качестве дополнительных классификаторов мы взяли линейную регрессию, svm, catboost. Из видео брались 6 случайных кадров, затем из них извлекались признаки, которые использовались для обучения классификаторов. Для финального предсказания использовался voting. 

Инференс выполнялся практически так же. Мы использовали 6 случайных кадров, обученную сеть и классификаторы. Получив ~0.85 на validation данных, мы отправили решение и ... даже не вошли в ЛБ, так как не влезли в ограничение по скорости. Оказалось, что чтение данных занимает слишком много времени. 

В итоге, нам пришлось сабмитить более слабые модели. Мы поменяли se_resnext на обычную se_resnet50. Решили сделать смешать validation и train, выделить из них новый train и validation, заново обучить все модели, но выкинуть caboost, и использовать случайны кадр из видео при инерефенсе. Данное решение дало 0.81 точности на валидации, но только 0.79 на ЛБ. На этом соревнование для нас и закончилось.

### Оптимизация

Именна та часть, на которую нужно было обратить пристальное внимание еще в самом начале. PyAV для быстрого декодирования видео решил проблему обучения сеток, но никак не инференса. Чтение 6 случайных кадров занимало 0.180 ms, и это даже без инференса. Чтение 3 случайных кадров, тоже выходило за граничные условия. В целом, декодирование видео на GPU - это ключ. Во-первых, можно использовать [nvvl](https://github.com/NVIDIA/nvvl) библиотеку от nvidia. Но из-за кучи зависимостей завести ее не удалось. Более того, при итерации по датасету, она держит все файлы открытыми. Когда есть всего пара сотен видео - это не проблема, но у нас их было куда больше. Один из хаков - увеличить лимит на количество открытых файловых дескрипторов системы, но ломать чужой девбокс не хотелось. Во-вторых, можно собрать ffmpeg с возможностью декодирования видео на GPU, но ffmpeg так же не собрался из-за кучи зависимостей. На другом железе все получилось, но из-за проблем с интернетом не вышло засабмитить докер.

 ## Послесловие

Эта статья была бы не полной без впечатлений от организации конкурса. Она была ужасна. AiChallenger позиционирует себя как глобальная платформа для AI соревнований. Наш конкурс был огранизован компанией meitu (美图). Началось все с того, что организаторы выложили информацию по докерку только 12-ого октября, когда соревнование началось 1-ого сентября. Все инструкции были на китайском. Со всеми вопросами орги посылали в чат в WeChat, где общение тоже велось на китайском. 

Сабмит докера у нас занимал более 7 часов, видимо, сказался золотой щит. Результаты сабмита, можно было посмотреть в профиле, где во время ошибки писалось очень информативное "Formation Error" или "Calculcation error". Чтобы получить детальную информацию, необходимо было написать оргам в WeChat, и они скидывали трейс. Такую информативность они объяснили тем, что в первый раз организовали соревнование, где нужно сабмитить докер и не знают как сделать лучше. Уже в конце соревнования мы написали гневный пост на форуме и орги пообещали отправлять на почту инфу по сабмиту. Но нам так ничего и не пришло. Орги дорабатывали платформу на лету и выкатили очень простое API, которое отдавало сырые данные и показывало не только точность всех сабмитов, но и скорость. В последний день соревнования, орги накинули 3 дополнительных сабмита, но мы даже не смогли их все запушить. 


В целом, это был интересный и положительный опыт штурма великой китайской стены. Конечно, нельзя валить все на оргов. Проанализировав наш подход, мы выявили проблемные места, но и из-за медлительности оргов, у нас было меньше времени на отладку и оптимизацию решения. Но в следующий раз мы будем лучфше подготовлены.